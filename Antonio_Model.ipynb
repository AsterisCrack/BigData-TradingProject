{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7SJ8ygP9MHi",
        "outputId": "787d00b8-c924-41cb-d007-24cda7235bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 229 kB in 3s (88.3 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A2ghCRE49MHj"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.window import Window\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yk-JNNG29MHj"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "appName = 'AntonioLorenzoFinalProject'\n",
        "spark = SparkSession.builder\\\n",
        "                    .appName(appName) \\\n",
        "                    .getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Y_ckgDAU9MHj",
        "outputId": "038a0287-85a7-49f3-aa53-67109da831a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7de09ef457e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5d07abb21e47:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>AntonioLorenzoFinalProject</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data from the csv file\n",
        "filename='uber.csv'\n",
        "df = spark.read.csv(filename, header=True, inferSchema=True)\n",
        "df = df.orderBy(\"date\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odidy-AZvW8T",
        "outputId": "50f1a682-2900-480b-d7d2-228acaf23cd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+----------+------------------+------------------+------------------+------------------+------------------+------------+-----+\n",
            "|    cik|ticker|      date|              open|              high|               low|             close|          adjclose|      volume|   rn|\n",
            "+-------+------+----------+------------------+------------------+------------------+------------------+------------------+------------+-----+\n",
            "|1543151|  UBER|2019-05-10|              42.0|              45.0|41.060001373291016| 41.56999969482422| 41.56999969482422|1.86322496E8|79688|\n",
            "|1543151|  UBER|2019-05-13|38.790000915527344|  39.2400016784668| 36.08000183105469|37.099998474121094|37.099998474121094|   7.94424E7|79687|\n",
            "|1543151|  UBER|2019-05-14|38.310001373291016|39.959999084472656|36.849998474121094|39.959999084472656|39.959999084472656|   4.66611E7|79686|\n",
            "|1543151|  UBER|2019-05-15|39.369998931884766|41.880001068115234| 38.95000076293945|41.290000915527344|41.290000915527344|   3.60861E7|79685|\n",
            "|1543151|  UBER|2019-05-16| 41.47999954223633|44.060001373291016|             41.25|              43.0|              43.0|   3.81155E7|79684|\n",
            "|1543151|  UBER|2019-05-17| 41.97999954223633|43.290000915527344| 41.27000045776367| 41.90999984741211| 41.90999984741211|   2.02257E7|79683|\n",
            "|1543151|  UBER|2019-05-20|41.189998626708984| 41.68000030517578|39.459999084472656| 41.59000015258789| 41.59000015258789|   2.92223E7|79682|\n",
            "|1543151|  UBER|2019-05-21|              42.0|  42.2400016784668|             41.25|              41.5|              41.5|   1.08029E7|79681|\n",
            "|1543151|  UBER|2019-05-22| 41.04999923706055|41.279998779296875|              40.5|             41.25|             41.25|   9089500.0|79680|\n",
            "|1543151|  UBER|2019-05-23| 40.79999923706055| 41.09000015258789| 40.02000045776367|40.470001220703125|40.470001220703125|   1.11199E7|79679|\n",
            "|1543151|  UBER|2019-05-24|41.279998779296875|  41.5099983215332|              40.5|  41.5099983215332|  41.5099983215332|   8786800.0|79678|\n",
            "|1543151|  UBER|2019-05-28| 41.70000076293945| 41.79999923706055|40.599998474121094| 40.95000076293945| 40.95000076293945|   1.33916E7|79677|\n",
            "|1543151|  UBER|2019-05-29| 40.52000045776367|40.709999084472656| 39.14699935913086|39.939998626708984|39.939998626708984|   1.40604E7|79676|\n",
            "|1543151|  UBER|2019-05-30| 40.06999969482422|40.380001068115234|              39.5| 39.79999923706055| 39.79999923706055|   2.64519E7|79675|\n",
            "|1543151|  UBER|2019-05-31|41.150001525878906| 41.56999969482422| 39.40999984741211| 40.40999984741211| 40.40999984741211|   2.32098E7|79674|\n",
            "|1543151|  UBER|2019-06-03| 40.74399948120117|41.849998474121094|  40.2400016784668|             41.25|             41.25|   1.66053E7|79673|\n",
            "|1543151|  UBER|2019-06-04|42.560001373291016|42.880001068115234| 40.70000076293945|             42.75|             42.75|   2.34321E7|79672|\n",
            "|1543151|  UBER|2019-06-05|42.869998931884766| 45.65999984741211|              42.5|              45.0|              45.0|   2.86096E7|79671|\n",
            "|1543151|  UBER|2019-06-06|              45.0|             45.75|44.279998779296875| 44.91999816894531| 44.91999816894531|   1.64037E7|79670|\n",
            "|1543151|  UBER|2019-06-07| 44.91999816894531| 45.66999816894531|44.130001068115234| 44.15999984741211| 44.15999984741211|   1.26547E7|79669|\n",
            "+-------+------+----------+------------------+------------------+------------------+------------------+------------------+------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.sql.functions import year, month, dayofmonth\n",
        "\n",
        "# Extract year, month, and day from the date column\n",
        "data_with_features = df.withColumn(\"year\", year(\"date\")) \\\n",
        "                                  .withColumn(\"month\", month(\"date\")) \\\n",
        "                                  .withColumn(\"day\", dayofmonth(\"date\"))\n",
        "data_with_features = data_with_features.orderBy(\"date\")\n",
        "\n",
        "# Calculate the index to make the split (80% train 20% test)\n",
        "split_index = int(0.8 * data_with_features.count())\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data = data_with_features.limit(split_index)\n",
        "test_data = data_with_features.subtract(train_data)\n",
        "\n",
        "# Define the features column\n",
        "feature_columns = ['year', 'month', 'day']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Define the DecisionTreeRegressor\n",
        "dtr = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"close\")\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[vector_assembler, dtr])\n",
        "\n",
        "# Define parameter grid for hyperparameter tuning\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(dtr.maxDepth, [3, 5, 7]) \\\n",
        "    .addGrid(dtr.maxBins, [3,5, 10]) \\\n",
        "    .build()\n",
        "\n",
        "# Define evaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# Create CrossValidator\n",
        "cross_val = CrossValidator(estimator=pipeline,\n",
        "                           estimatorParamMaps=param_grid,\n",
        "                           evaluator=evaluator,\n",
        "                           numFolds=10,\n",
        "                           seed=42)      # Set seed for reproducibility\n",
        "\n",
        "# Fit the model\n",
        "cv_model = cross_val.fit(train_data)\n",
        "\n",
        "# Get best model\n",
        "best_model = cv_model.bestModel\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = best_model.transform(test_data)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Root Mean Squared Error (RMSE) on test data =\", rmse)\n"
      ],
      "metadata": {
        "id": "JSiLJifDvuO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [{p.name: v for p, v in m.items()} for m in cv_model.getEstimatorParamMaps()]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([\n",
        "    {cv_model.getEvaluator().getMetricName(): metric, **ps}\n",
        "    for ps, metric in zip(params, cv_model.avgMetrics)\n",
        "])"
      ],
      "metadata": {
        "id": "6rAe897ZwFTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Split data into train and test sets\n",
        "split_index = int(data_with_features.count() - 7)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data = data_with_features.limit(split_index)\n",
        "test_data = data_with_features.subtract(train_data)\n",
        "\n",
        "# Define the features column\n",
        "feature_columns = ['year', 'month', 'day']\n",
        "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Define the model with the best cv values\n",
        "best_dtr = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"close\", maxDepth=7, maxBins= 10)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[vector_assembler, best_dtr])\n",
        "\n",
        "# Fit the model on the entire dataset except the last 7 days\n",
        "start_time = time.time()\n",
        "model = pipeline.fit(train_data)\n",
        "end_time = time.time()\n",
        "print(\"Training time: {:.2f} seconds\".format(end_time - start_time))\n",
        "\n",
        "# Make predictions on the entire dataset\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Define evaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_mae = RegressionEvaluator(labelCol=\"close\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "evaluator_r2 = RegressionEvaluator(labelCol=\"close\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "mae = evaluator_mae.evaluate(predictions)\n",
        "r2 = evaluator_r2.evaluate(predictions)\n",
        "\n",
        "print(\"Model results for last 7 days\")\n",
        "print(\"Root Mean Squared Error (RMSE) on test data =\", rmse)\n",
        "print(\"Mean Absolute Error (MAE) on test data =\", mae)\n",
        "print(\"R-squared (R2) on test data =\", r2)"
      ],
      "metadata": {
        "id": "DkcDPx-RwIV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.show()"
      ],
      "metadata": {
        "id": "5Kvg9777wNho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data.orderBy(\"date\")\n",
        "test_data.show()"
      ],
      "metadata": {
        "id": "bPDXLuAjwTJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "predictions = predictions.orderBy(\"date\")\n",
        "\n",
        "# Collecting data from DataFrame columns as lists\n",
        "dates = predictions.select(\"date\").rdd.flatMap(lambda x: x).collect()\n",
        "predictions_values = predictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
        "real_values = predictions.select(\"close\").rdd.flatMap(lambda x: x).collect()\n",
        "max_val = max(real_values) + 5\n",
        "min_val = min(real_values) - 5\n",
        "valores_random = [random.uniform(min_val, max_val) for _ in range(7)]\n",
        "\n",
        "# Plotting the data\n",
        "plt.plot(dates, predictions_values, label='Predicciones', linestyle='-', marker='o', color='b')\n",
        "plt.plot(dates, real_values, label='Real', linestyle='--', marker='s', color='r')\n",
        "plt.plot(dates, valores_random, label='Random', linestyle=':', marker='s', color='g')\n",
        "\n",
        "# Labels and legend\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Valor')\n",
        "plt.title('Predicciones vs Real')\n",
        "plt.legend()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "07QiTT82wUaA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}